{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\conda\\ANACONDA\\envs\\cuda_test\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders import PyMuPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#from langchain.lims import CTransformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "#Database\n",
    "import chromadb\n",
    "\n",
    "\n",
    "import pinecone\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "def configure():\n",
    "    load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cuda\")\n",
    "\n",
    "# Convert chunks to vector embeddings using GPU\n",
    "#embeddings = embedding_model.encode(texts, convert_to_tensor=True, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = client.get_collection(\"medical_chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrive the Relevant Chunks from the ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant Chunks: ['Question/nWhat are the symptoms of Diabetes ?Answer/nMany people with diabetes experience one or more symptoms, including extreme thirst or hunger, a frequent need to urinate and/or fatigue. Some lose weight without trying. Additional signs include sores that heal slowly, dry, itchy skin, loss of feeling or tingling in the feet and blurry eyesight. Some people with diabetes, however, have no symptoms at all.', 'urination  - feeling very hungry or tired  - losing weight without trying  - having sores that heal slowly  - having dry, itchy skin  - loss of feeling or tingling in the feet  - having blurry eyesight. being very thirsty frequent urination feeling very hungry or tired losing weight without trying having sores that heal slowly having dry, itchy skin loss of feeling or tingling in the feet having blurry eyesight. Signs of type 1 diabetes usually develop over a short period of time. The signs for', 'Question/nWhat are the symptoms of Your Guide to Diabetes: Type 1 and Type 2 ?Answer/nThe signs and symptoms of diabetes are\\n                \\n- being very thirsty  - urinating often  - feeling very hungry  - feeling very tired  - losing weight without trying  - sores that heal slowly  - dry, itchy skin  - feelings of pins and needles in your feet  - losing feeling in your feet  - blurry eyesight']\n"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_chunks(query, top_k=3,):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True, device=\"cuda\").tolist()\n",
    "\n",
    "    # Perform similarity search\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "    retrieved_texts = results[\"documents\"][0]  # Top-k documents\n",
    "    return retrieved_texts\n",
    "\n",
    "# Example Query\n",
    "query = \"What are the symptoms of diabetes?\"\n",
    "retrieved_texts = retrieve_relevant_chunks(query)\n",
    "print(\"Relevant Chunks:\", retrieved_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the LLAMA 2(7B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convert bytes to GB\n",
    "gpu_layers = min(50, int(gpu_mem * 5))  # Adjust dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Unterscheidung between type 1 and type 2 diabetes? \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "local_model_path = \"E:/Project/LLM_HEALTH_LOCAL_DATA/Model/Model/\"\n",
    "\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Load GGUF model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"E:/Project/LLM_HEALTH_LOCAL_DATA/Model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "    model_type=\"llama\",\n",
    "    gpu_layers=gpu_layers  # Adjust based on GPU memory\n",
    ")\n",
    "\n",
    "# Generate text\n",
    "response = model(\"What are the symptoms of diabetes?\", max_new_tokens=200)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline(query):\n",
    "    query_embedding = embedding_model.encode(query).tolist()\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "    # Step 2: Retrieve relevant documents from ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=2 \n",
    "    )\n",
    "    retrieved_docs = results[\"documents\"][0] if results[\"documents\"] else []\n",
    "\n",
    "    # Step 3: Prepare context for LLAMA 2\n",
    "    if retrieved_docs:\n",
    "        context = \"\\n\".join(retrieved_docs)\n",
    "    else:\n",
    "        context = \"No relevant documents were found in the database.\"\n",
    "\n",
    "    # Step 4: Generate response using LLAMA 2\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    data = {\n",
    "    \"model\" : \"llama2\",\n",
    "    \"prompt\" : prompt,\n",
    "    \"stream\" : False\n",
    "    }   \n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\" : \"application/json\"\n",
    "    }\n",
    "    # âœ… FIX: Use `model(prompt)` instead of `model.generate()`\n",
    "    #response = model(prompt, max_new_tokens=200)  # Direct function call\n",
    "    response = requests.post(url= url, json=data, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        value = response.json()\n",
    "        print(value[\"response\"])\n",
    "    \n",
    "    \n",
    "    return value[\"response\"]\n",
    "\n",
    "# Test RAG pipeline\n",
    "query = \"I feel like vomiting?\"\n",
    "response = rag_pipeline(query)\n",
    "print(\"RAG Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 01:33:13.824 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-05 01:33:14.014 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\rayan\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-03-05 01:33:14.015 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-05 01:33:14.015 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-05 01:33:14.015 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-05 01:33:14.015 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-05 01:33:14.018 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-05 01:33:14.018 Session state does not function when running a script without `streamlit run`\n",
      "2025-03-05 01:33:14.018 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-05 01:33:14.019 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-05 01:33:14.019 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-05 01:33:14.019 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-05 01:33:14.020 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-05 01:33:14.020 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-03-05 01:33:14.020 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.title(\"ðŸ’¡ Medical Chatbot with LLAMA 2 & ChromaDB\")\n",
    "\n",
    "user_query = st.text_input(\"Ask a medical question:\")\n",
    "\n",
    "if st.button(\"Get Answer\"):\n",
    "    if user_query:\n",
    "        response = rag_pipeline(query)\n",
    "        st.write(response)\n",
    "    else:\n",
    "        st.warning(\"Please enter a question!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
